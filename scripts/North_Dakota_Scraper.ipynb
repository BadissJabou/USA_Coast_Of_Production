{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactored North Dakota Crop Budget Scraper\n",
    "## Improved Performance, Automation, and Error Handling\n",
    "\n",
    "This notebook extracts crop budget data from NDSU website with modular, efficient code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import logging\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch the HTML content of a webpage with proper headers.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to fetch.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[str]: The HTML content if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        logger.info(f\"Successfully fetched {url}\")\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Failed to fetch {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(soup: BeautifulSoup) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract all href and text pairs from anchor tags within span elements.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML content.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: List of (href, text) tuples.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    for span in soup.find_all(\"span\"):\n",
    "        for a in span.find_all(\"a\", href=True):\n",
    "            links.append((a[\"href\"], a.text.strip()))\n",
    "    logger.info(f\"Extracted {len(links)} links from page\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(links: List[Tuple[str, str]], min_year: int = 2023) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Filter links to only XLS files from specified year onwards and extract metadata.\n",
    "    \n",
    "    Args:\n",
    "        links (List[Tuple[str, str]]): List of (href, text) tuples.\n",
    "        min_year (int): Minimum year to include.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, str, str]]: List of (full_url, year, location) tuples.\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for href, text in links:\n",
    "        if href.lower().endswith('.xls'):\n",
    "            try:\n",
    "                year = int(text[:4])\n",
    "                if year >= min_year:\n",
    "                    location = text.split('ND')[0][4:].strip() if 'ND' in text else text\n",
    "                    full_url = f\"https://www.ndsu.edu{href}\"\n",
    "                    filtered.append((full_url, str(year), location))\n",
    "            except (ValueError, IndexError):\n",
    "                logger.warning(f\"Skipping invalid link text: {text}\")\n",
    "                continue\n",
    "    logger.info(f\"Filtered to {len(filtered)} valid XLS links for {min_year}+\")\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_units(items: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Assign appropriate units based on item names.\n",
    "    \n",
    "    Args:\n",
    "        items (pd.Series): Series of item names.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of corresponding units.\n",
    "    \"\"\"\n",
    "    units = []\n",
    "    unit_map = {\n",
    "        \"Market Yield\": \"bu/acre\",\n",
    "        \"Market Price\": \"$/bu\",\n",
    "        \"Market Price + LDP:\": \"$/bu\"\n",
    "    }\n",
    "    for item in items:\n",
    "        item_str = str(item).strip()\n",
    "        units.append(unit_map.get(item_str, \"$/acre\"))\n",
    "    return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_excel_sheet(url: str, sheet: str, location: str, year: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a single Excel sheet and return cleaned DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the Excel file.\n",
    "        sheet (str): Sheet name to process.\n",
    "        location (str): Location metadata.\n",
    "        year (str): Year metadata.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: Cleaned DataFrame or None if failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(url, sheet_name=sheet, usecols=\"A:B\", nrows=29)\n",
    "        if df.empty:\n",
    "            logger.warning(f\"Empty sheet {sheet} in {url}\")\n",
    "            return None\n",
    "        df = df.rename(columns={df.columns[0]: \"Item\", df.columns[1]: \"Value\"})\n",
    "        df = df.dropna()\n",
    "        df[\"Item\"] = df[\"Item\"].astype(str).str.replace(r\"-\", \"\", regex=True)\n",
    "        df = df.assign(\n",
    "            Location=f\"ND {location}\",\n",
    "            Source=\"NDSU\",\n",
    "            Commodity=sheet,\n",
    "            Year=year\n",
    "        )\n",
    "        df[\"Unit\"] = assign_units(df[\"Item\"])\n",
    "        logger.info(f\"Processed {sheet} from {url}: {len(df)} rows\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to process {sheet} from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_north_dakota_data(url: str, min_year: int = 2006) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function to extract North Dakota crop budget data.\n",
    "    \n",
    "    Args:\n",
    "        url (str): Base URL for scraping.\n",
    "        min_year (int): Minimum year to include.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all extracted data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data extraction...\")\n",
    "    \n",
    "    page_content = fetch_page(url)\n",
    "    if not page_content:\n",
    "        logger.error(\"Failed to fetch main page\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "    links = extract_links(soup)\n",
    "    filtered_links = filter_links(links, min_year)\n",
    "    \n",
    "    if not filtered_links:\n",
    "        logger.warning(\"No valid links found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    commodities = [\"Corn\", \"Soy\", \"Soybean\"]  # Corrected commodities\n",
    "    all_data = []\n",
    "    \n",
    "    for link_url, year, location in filtered_links:\n",
    "        for crop in commodities:\n",
    "            df = process_excel_sheet(link_url, crop, location, year)\n",
    "            if df is not None:\n",
    "                all_data.append(df)\n",
    "    \n",
    "    if all_data:\n",
    "        result = pd.concat(all_data, ignore_index=True)\n",
    "        logger.info(f\"Extracted {len(result)} total rows of data\")\n",
    "        return result\n",
    "    else:\n",
    "        logger.warning(\"No data extracted\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "url = \"https://www.ndsu.edu/agriculture/ag-hub/ag-topics/farm-management/crop-economics/projected-crop-budgets\"\n",
    "data = extract_north_dakota_data(url, 2006)\n",
    "\n",
    "if not data.empty:\n",
    "    # Transform year format\n",
    "    def transform_year(year):\n",
    "        try:\n",
    "            y = int(year)\n",
    "            return f\"{y}/{y+1}\"\n",
    "        except ValueError:\n",
    "            return year\n",
    "    \n",
    "    data[\"Year\"] = data[\"Year\"].apply(transform_year)\n",
    "    output_path = \"North_Dakota.xlsx\"\n",
    "    data.to_excel(output_path, index=False)\n",
    "    logger.info(f\"Data saved to {output_path}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(data.head())\n",
    "    print(f\"\\nUnique locations: {sorted(data['Location'].unique())}\")\n",
    "    print(f\"Total rows: {len(data)}\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
